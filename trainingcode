!pip install torch transformers datasets scikit-learn nltk flask
import torch
from torch.utils.data import DataLoader, Dataset
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string

# Download NLTK data
nltk.download('punkt')
nltk.download('stopwords')

# Load the IMDB dataset
from datasets import load_dataset
dataset = load_dataset('imdb')

# Convert to pandas dataframe
df_train = pd.DataFrame(dataset['train'])
df_test = pd.DataFrame(dataset['test'])

# Preprocess text: tokenization, stop words removal, and punctuation removal
stop_words = set(stopwords.words('english'))
punctuation = set(string.punctuation)

def preprocess_text(text):
    tokens = word_tokenize(text.lower())
    filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words and word not in punctuation]
    return ' '.join(filtered_tokens)

df_train['text'] = df_train['text'].apply(preprocess_text)
df_test['text'] = df_test['text'].apply(preprocess_text)

# Map labels to scores (0 or 1 to a scale of 0-10)
df_train['label'] = df_train['label'] * 10
df_test['label'] = df_test['label'] * 10

# Split the data into two parts
df_train_part1, df_train_part2 = train_test_split(df_train, test_size=0.5, random_state=42)
class SentimentDataset(Dataset):
    def _init_(self, texts, scores, tokenizer, max_len):
        self.texts = texts
        self.scores = scores
        self.tokenizer = tokenizer
        self.max_len = max_len

    def _len_(self):
        return len(self.texts)

    def _getitem_(self, idx):
        text = self.texts[idx]
        score = self.scores[idx]
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            return_token_type_ids=False,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt',
        )
        return {
            'text': text,
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'score': torch.tensor(score, dtype=torch.float)
        }

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def get_data_loader(df, tokenizer, batch_size=8, max_len=128):
    dataset = SentimentDataset(df['text'].to_numpy(), df['label'].to_numpy(), tokenizer, max_len=max_len)
    return DataLoader(dataset, batch_size=batch_size, shuffle=True)

def train_model(model, data_loader, loss_fn, optimizer, device, n_examples):
    model = model.train()
    losses = []
    for data in data_loader:
        input_ids = data['input_ids'].to(device)
        attention_mask = data['attention_mask'].to(device)
        scores = data['score'].to(device)
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        preds = outputs.logits.squeeze()
        loss = loss_fn(preds, scores)
        losses.append(loss.item())
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
    return np.mean(losses)

def eval_model(model, data_loader, loss_fn, device, n_examples):
    model = model.eval()
    losses = []
    with torch.no_grad():
        for data in data_loader:
            input_ids = data['input_ids'].to(device)
            attention_mask = data['attention_mask'].to(device)
            scores = data['score'].to(device)
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            preds = outputs.logits.squeeze()
            loss = loss_fn(preds, scores)
            losses.append(loss.item())
    return np.mean(losses)
# Initialize and train the model on part 1
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)  # Regression
model = model.to(device)

optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)
loss_fn = torch.nn.MSELoss().to(device)  # Mean Squared Error Loss for regression

train_data_loader_part1 = get_data_loader(df_train_part1, tokenizer)

EPOCHS = 3

for epoch in range(EPOCHS):
    print(f'Epoch {epoch + 1}/{EPOCHS} (Part 1)')
    train_loss = train_model(model, train_data_loader_part1, loss_fn, optimizer, device, len(df_train_part1))
    print(f'Train loss: {train_loss}')
# Save the model and tokenizer after training on part 1
model.save_pretrained('sentiment_model_part1')
tokenizer.save_pretrained('sentiment_tokenizer')
# Load the model and continue training on part 2
model = BertForSequenceClassification.from_pretrained('sentiment_model_part1', num_labels=1)
model = model.to(device)

train_data_loader_part2 = get_data_loader(df_train_part2, tokenizer)

for epoch in range(EPOCHS):
    print(f'Epoch {epoch + 1}/{EPOCHS} (Part 2)')
    train_loss = train_model(model, train_data_loader_part2, loss_fn, optimizer, device, len(df_train_part2))
    print(f'Train loss: {train_loss}')
# Save the final model after training on part 2
model.save_pretrained('sentiment_model_final')
tokenizer.save_pretrained('sentiment_tokenizer_final')
